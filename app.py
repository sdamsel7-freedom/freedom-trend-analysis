import streamlit as st
import pandas as pd
import requests
import json
from datetime import datetime
import io

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="í”„ë¦¬ë¤ íŠ¸ë Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸƒâ€â™‚ï¸ Freedom Trend Analysis Dashboard")

# 2. ë³´ì•ˆ ì„¤ì • (Secrets)
try:
    CLIENT_ID = st.secrets["NAVER_CLIENT_ID"]
    CLIENT_SECRET = st.secrets["NAVER_CLIENT_SECRET"]
except KeyError:
    st.error("ì˜¤ë¥˜: Streamlit Secretsì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# 3. Naver API í˜¸ì¶œ í•¨ìˆ˜ (ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥ ê¸°ëŠ¥ ì¶”ê°€)
def get_api_data(keyword_groups, gender):
    url = "https://openapi.naver.com/v1/datalab/search"
    headers = {
        "X-Naver-Client-Id": CLIENT_ID,
        "X-Naver-Client-Secret": CLIENT_SECRET,
        "Content-Type": "application/json"
    }
    body = {
        "startDate": "2024-01-01",
        "endDate": datetime.now().strftime("%Y-%m-%d"),
        "timeUnit": "month",
        "keywordGroups": keyword_groups,
        "device": "",
        "ages": ["3", "4", "5", "6", "7"],
        "gender": gender
    }
    response = requests.post(url, headers=headers, data=json.dumps(body))
    if response.status_code == 200:
        res_json = response.json()
        data_list = []
        for group in res_json['results']:
            # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            if 'data' in group and group['data']:
                for entry in group['data']:
                    data_list.append({
                        'Date': entry['period'],
                        'Keyword_Group': group['title'],
                        'Ratio': entry['ratio'],
                        'Gender': 'Male' if gender == 'm' else 'Female'
                    })
        return pd.DataFrame(data_list)
    else:
        # API ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ìì—ê²Œ ì´ìœ ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.
        st.sidebar.error(f"API í˜¸ì¶œ ì‹¤íŒ¨ ({response.status_code}): {response.text}")
        return pd.DataFrame()

# 4. ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“ ë°ì´í„° ê´€ë¦¬")
    uploaded_file = st.file_uploader("ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])

# 5. ë©”ì¸ ë¡œì§
if uploaded_file:
    df_input = pd.read_excel(uploaded_file)
    
    if 'GroupName' in df_input.columns:
        all_groups = []
        for _, row in df_input.iterrows():
            g_name = str(row['GroupName']).strip()
            # [ì¶”ê°€] ë¹ˆ í–‰ì´ê±°ë‚˜ '*'ë¡œ ì‹œì‘í•˜ëŠ” ë©”ëª¨ í–‰ì€ ê±´ë„ˆëœë‹ˆë‹¤.
            if not g_name or g_name.startswith('*') or g_name == "nan":
                continue
                
            kw_val = str(row['Keywords']).strip() if 'Keywords' in df_input.columns and pd.notnull(row['Keywords']) else ""
            keywords = [k.strip() for k in kw_val.split(',')] if kw_val and kw_val != "nan" else [g_name]
            all_groups.append({"groupName": g_name, "keywords": keywords})

        if not all_groups:
            st.error("ë¶„ì„í•  ìœ íš¨í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        anchor_group = all_groups[0]
        anchor_name = anchor_group['groupName']
        other_groups = all_groups[1:]

        if st.sidebar.button("ğŸš€ ë¶„ì„ ì‹œì‘ (Run Analysis)"):
            final_df = pd.DataFrame()
            reference_data = pd.DataFrame()
            progress = st.progress(0)
            
            batch_size = 4
            for i in range(0, len(other_groups) if other_groups else 1, batch_size):
                chunk = other_groups[i:i+batch_size]
                current_batch = [anchor_group] + chunk
                batch_res = pd.concat([get_api_data(current_batch, 'm'), get_api_data(current_batch, 'f')], ignore_index=True)
                
                # [í•µì‹¬ ìˆ˜ì •] ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ì•ˆì „ì¥ì¹˜
                if batch_res.empty:
                    st.warning(f"{current_batch} ê·¸ë£¹ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                if i == 0 or reference_data.empty:
                    reference_data = batch_res[batch_res['Keyword_Group'] == anchor_name].copy()
                    final_df = batch_res
                else:
                    curr_anchor = batch_res[batch_res['Keyword_Group'] == anchor_name].copy()
                    if not curr_anchor.empty:
                        scale_merge = pd.merge(curr_anchor, reference_data, on=['Date', 'Gender'], suffixes=('_curr', '_ref'))
                        if not scale_merge.empty:
                            scale_merge['Factor'] = scale_merge['Ratio_ref'] / scale_merge['Ratio_curr']
                            batch_res = pd.merge(batch_res, scale_merge[['Date', 'Gender', 'Factor']], on=['Date', 'Gender'])
                            batch_res['Ratio'] = batch_res['Ratio'] * batch_res['Factor']
                            final_df = pd.concat([final_df, batch_res[batch_res['Keyword_Group'] != anchor_name]], ignore_index=True)
                
                progress.progress(min((i + batch_size) / (len(other_groups) + 1) if other_groups else 1.0, 1.0))

            if not final_df.empty:
                st.session_state['analysis_result'] = final_df
                st.session_state['anchor_name'] = anchor_name
                st.success("ë¶„ì„ ì™„ë£Œ!")
            else:
                st.error("ë„¤ì´ë²„ì—ì„œ ê²€ìƒ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë‚˜ API ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        # ë¶„ì„ ê²°ê³¼ í‘œì‹œ ì„¹ì…˜ (ê¸°ì¡´ê³¼ ë™ì¼)
        if 'analysis_result' in st.session_state:
            res_df = st.session_state['analysis_result']
            # ... (ì¤‘ëµ: í•„í„° ë° ê·¸ë˜í”„ ì¶œë ¥ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)
